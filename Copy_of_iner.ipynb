{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SakshiUK/BharatInternStockPricePrediction/blob/main/Copy_of_iner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epHoH0x14owK",
        "outputId": "26ffab04-296b-40bd-c5f5-33818da5a603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, quote\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import cmudict\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('cmudict')\n",
        "\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        title = soup.title.text.strip() if soup.title else \"Untitled\"\n",
        "        article_content = soup.find('div', class_='td-post-content tagdiv-type')\n",
        "        if article_content:\n",
        "            article_text = '\\n'.join(p.text.strip() for p in article_content.find_all('p'))\n",
        "        else:\n",
        "            article_text = \"Article content not found.\"\n",
        "\n",
        "        return title, article_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting content from {url}: {e}\")\n",
        "        return \"Untitled\", None\n",
        "\n",
        "\n",
        "def save_to_text_file(file_name, title, article_text):\n",
        "    with open(file_name, 'w', encoding='utf-8', errors='replace') as file:\n",
        "        file.write(f\"Title: {title}\\n\\n\")\n",
        "        file.write(article_text)\n",
        "\n",
        "def read_stop_words(stop_word_files):\n",
        "    stop_words = set()\n",
        "    for file_name in stop_word_files:\n",
        "        with open(file_name, 'r', encoding='iso-8859-1') as file:\n",
        "            stop_words.update(word.strip() for word in file.readlines())\n",
        "    return stop_words\n",
        "\n",
        "def read_positive_negative_words(positive_file, negative_file):\n",
        "    with open(positive_file, 'r', encoding='ISO-8859-1') as file:\n",
        "        positive_words = [word.strip() for word in file.readlines()]\n",
        "\n",
        "    with open(negative_file, 'r', encoding='ISO-8859-1') as file:\n",
        "        negative_words = [word.strip() for word in file.readlines()]\n",
        "\n",
        "    return positive_words, negative_words\n",
        "\n",
        "\n",
        "def clean_text(text, stop_words):\n",
        "    cleaned_text=' '.join(word.strip(string.punctuation) for word in text.split() if word.lower() not in stop_words)\n",
        "    return cleaned_text\n",
        "\n",
        "def calculate_positive_score(text,positive_words):\n",
        "    tokens=word_tokenize(text)\n",
        "    positive_score=sum(1 for token in tokens if token.lower() in positive_words)\n",
        "    return positive_score\n",
        "\n",
        "def calculate_negative_score(text,negative_words):\n",
        "    tokens=word_tokenize(text)\n",
        "    negative_score=sum(-1 for token in tokens if token.lower() in negative_words)\n",
        "    return negative_score\n",
        "\n",
        "def calculate_polarity_score(text,positive_words,negative_words):\n",
        "    tokens=word_tokenize(text)\n",
        "    negative_score=sum(-1 for token in tokens if token.lower() in negative_words)\n",
        "    positive_score=sum(-1 for token in tokens if token.lower() in positive_words)\n",
        "    polarity_score=(positive_score - negative_score)/ ((positive_score + negative_score) + 0.000001)\n",
        "    return  polarity_score\n",
        "\n",
        "def calculate_subjectivity_score(text,positive_words,negative_words):\n",
        "    tokens=word_tokenize(text)\n",
        "    total_words=len(tokens)\n",
        "    negative_score=sum(-1 for token in tokens if token.lower() in negative_words)\n",
        "    positive_score=sum(-1 for token in tokens if token.lower() in positive_words)\n",
        "    subjectivity_score = (positive_score + negative_score)/ ((total_words) + 0.000001)\n",
        "    return subjectivity_score\n",
        "\n",
        "def calculate_complex_words(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pronouncing_dict = cmudict.dict()\n",
        "    complex_words = [word for word in tokens if word.isalpha() and count_syllables(word) > 2]\n",
        "    num_complex_words = len(complex_words)\n",
        "    return num_complex_words\n",
        "\n",
        "def calculate_average_word_length(text):\n",
        "    tokens=word_tokenize(text)\n",
        "    no_of_words=len(tokens)\n",
        "    sum_of_chars=sum(len(token) for token in tokens)\n",
        "    average_word_length=sum_of_chars/no_of_words\n",
        "    return average_word_length\n",
        "\n",
        "def calculate_sentences(text):\n",
        "    tokens=sent_tokenize(text)\n",
        "    no_of_sentences=len(tokens)\n",
        "    return no_of_sentences\n",
        "\n",
        "def calculate_avg_sent_length(text):\n",
        "    no_of_words=len(word_tokenize(text))\n",
        "    no_of_sentences=calculate_sentences(text)\n",
        "    average_sentence_length=no_of_words/no_of_sentences\n",
        "    return average_sentence_length\n",
        "\n",
        "def calculate_words(text):\n",
        "    tokens=word_tokenize(text)\n",
        "    num_of_words=len(tokens)\n",
        "    return num_of_words\n",
        "\n",
        "def calculate_percentage_complex_words(text):\n",
        "    num_complex_words=calculate_complex_words(text)\n",
        "    no_of_words=calculate_words(text)\n",
        "    percentage_complex_words=num_complex_words/no_of_words\n",
        "    return percentage_complex_words\n",
        "\n",
        "def calculate_fog_index(text):\n",
        "    avg_sent_len=calculate_avg_sent_length(text)\n",
        "    per_complex_words=calculate_percentage_complex_words(text)\n",
        "    fog_index=0.4*(avg_sent_len + per_complex_words)\n",
        "    return fog_index\n",
        "\n",
        "def calculate_personal_pronouns(text):\n",
        "    pronoun_pattern=re.compile(r'\\b(?:I|we|my|ours|us)\\b',flags=re.IGNORECASE)\n",
        "    pronoun_matches=pronoun_pattern.findall(text)\n",
        "    pronoun_count=len(pronoun_matches)\n",
        "    return pronoun_count\n",
        "\n",
        "def count_syllables(word):\n",
        "    vowels =\"aeiouy\"\n",
        "    word=word.lower()\n",
        "    count=0\n",
        "    prev_char=None\n",
        "\n",
        "    for char in word:\n",
        "        if char in vowels and (prev_char is None or prev_char not in vowels):\n",
        "            count+=1\n",
        "        prev_char=char\n",
        "\n",
        "    if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
        "        count-=1\n",
        "\n",
        "    return max(count,1)\n",
        "\n",
        "def calculate_syllables(text):\n",
        "    tokens=word_tokenize(text)\n",
        "    syllables_count=sum(count_syllables(token) for token in tokens)\n",
        "    return syllables_count\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    df = pd.read_excel('Input.xlsx')\n",
        "    newdf=pd.read_excel('Output Data Structure.xlsx')\n",
        "    stop_word_files=['/content/StopWords_Auditor.txt','/content/StopWords_Currencies.txt','/content/StopWords_DatesandNumbers.txt','/content/StopWords_Generic.txt','/content/StopWords_GenericLong.txt','/content/StopWords_Geographic.txt','/content/StopWords_Names.txt']\n",
        "    stop_words = read_stop_words(stop_word_files)\n",
        "    positive_file = '/content/positive-words.txt'\n",
        "    negative_file = '/content/StopWords_Names.txt'\n",
        "    positive_words, negative_words = read_positive_negative_words(positive_file, negative_file)\n",
        "    for index, row in df.iterrows():\n",
        "        url = row['URL']\n",
        "        url_id =  row['URL_ID']\n",
        "        title, article_text = extract_article_text(url)\n",
        "\n",
        "        if title and article_text:\n",
        "\n",
        "            cleaned_article_text = clean_text(article_text, stop_words)\n",
        "\n",
        "            filtered_positive_words = [word for word in positive_words if word.lower() not in stop_words]\n",
        "            filtered_negative_words = [word for word in negative_words if word.lower() not in stop_words]\n",
        "\n",
        "            positive_score = calculate_positive_score(cleaned_article_text, positive_words)\n",
        "            negative_score = calculate_negative_score(cleaned_article_text, positive_words)\n",
        "            polarity_score = calculate_polarity_score(cleaned_article_text, positive_words, negative_words)\n",
        "            subjectivity_score = calculate_subjectivity_score(cleaned_article_text, positive_words, negative_words)\n",
        "\n",
        "            average_word_length=calculate_average_word_length(cleaned_article_text)\n",
        "\n",
        "            pronoun_count=calculate_personal_pronouns(cleaned_article_text)\n",
        "\n",
        "            syllables_count=calculate_syllables(cleaned_article_text)\n",
        "\n",
        "            word_count=len(word_tokenize(cleaned_article_text))\n",
        "\n",
        "            num_complex_words=calculate_complex_words(cleaned_article_text)\n",
        "\n",
        "            num_words_per_sent=word_count/num_complex_words\n",
        "\n",
        "            average_sentence_length=calculate_avg_sent_length(cleaned_article_text)\n",
        "            percentage_complex_words=calculate_percentage_complex_words(cleaned_article_text)\n",
        "            fog_index=calculate_fog_index(cleaned_article_text)\n",
        "\n",
        "            file_name = f\"{url_id}_cleaned.txt\"\n",
        "            save_to_text_file(file_name, title, cleaned_article_text)\n",
        "\n",
        "            existing_sheet = pd.read_excel('Output Data Structure.xlsx')\n",
        "\n",
        "            # ['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
        "      #  'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n",
        "      #  'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
        "      #  'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
        "      #  'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
        "\n",
        "            existing_sheet.at[index, 'POSITIVE SCORE'] = positive_score\n",
        "            existing_sheet.at[index, 'NEGATIVE SCORE'] = negative_score\n",
        "            existing_sheet.at[index, 'POLARITY SCORE'] = polarity_score\n",
        "            existing_sheet.at[index, 'SUBJECTIVITY SCORE'] = subjectivity_score\n",
        "            existing_sheet.at[index, 'AVG SENTENCE LENGTH'] = average_sentence_length\n",
        "            existing_sheet.at[index, 'PERCENTAGE OF COMPLEX WORDS'] = percentage_complex_words\n",
        "            existing_sheet.at[index, 'FOG INDEX'] = fog_index\n",
        "            existing_sheet.at[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = num_words_per_sent\n",
        "            existing_sheet.at[index, 'COMPLEX WORD COUNT'] = num_complex_words\n",
        "            existing_sheet.at[index, 'WORD COUNT'] = word_count\n",
        "            existing_sheet.at[index, 'SYLLABLE PER WORD'] = syllables_count\n",
        "            existing_sheet.at[index, 'PERSONAL PRONOUNS'] = pronoun_count\n",
        "            existing_sheet.at[index, 'AVG WORD LENGTH'] = average_word_length\n",
        "\n",
        "            existing_sheet.to_excel('Output Data Structure.xlsx', index=False)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UQbfEd9Ptfyp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsuQc6Wx2ATIWhhN7KTIXx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}